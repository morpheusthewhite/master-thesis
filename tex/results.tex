\chapter{Results and Analysis}
\label{ch:resultsAndAnalysis}

\section{Data collection and generation}%
\label{sec:data_collection_and_generation}

We now present how real-world data is retrieved and preprocessed and
define techniques for generatic synthetic data.

\subsection{Collection and preprocessing}%
\label{sub:collection_and_preprocessing}

Datasets are built over two social medias: Twitter and Reddit; the data
collection process, consequently, slightly differs between them.

\paragraph{Twitter}%
\label{par:twitter-data}

Twitter's \emph{Interaction Graphs} are mainly built starting from the tweets
of profiles associated to well-known news sources, like The
New York Times or Fox News, that tipically post links to their articles:
the set of shared URLs are the contents $\mathcal{C} $ of the corresponding interaction graph.

Each time another user tweets one of these URLs (as seen in
\autoref{fig:twitter-thread}) he
will create another thread related to the same content, and all the
replies it receives will be part of this new thread.

Twitter data is retrieved with the help of Tweepy \cite{tweepy}, a Python
library for accessing the Twitter API, which has been patched for using some
features available only in the beta of the new v2 Twitter API.

In order to validate our methods, we also construct a datasets in which users are labeled either as
\emph{democrats} or \emph{republican} \footnotemark. This is done by looking at the people a
certain user $v_i$ follows: for each account $v_j$ followed by $v_i$, if $v_j$ is a political representative, then we can
retrieve from Wikipedia the party to which he belongs to (see, for
example, \autoref{fig:tex/img/wikipedia-elected}). Then, user $v_i$ get assigned a
label according to the party of the majority of the users $v_j$ he
follows.

\footnotetext{We chose to use these two labels since the news sources we analyze for this purpose are based in U.S..
	Also, we think political discussion are the main source of controversial
	content and so it is the most interesting criteria according to which users
	can be differentiated}

\autoref{fig:tex/img/wikipedia-elected}).
\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{tex/img/wikipedia-elected.png}
	\caption[Example Wikipedia entry]{Wikipedia entry associated to Alexandria Ocasio-Cortez, a member
		of the U.S. House of Representatives.}%
	\label{fig:tex/img/wikipedia-elected}
\end{figure}

\paragraph{Reddit.}%
\label{par:reddit}

Differently from Twitter, Reddit focuses on subreddits, which are pages
collecting posts of users about a specific topic (e.g. r/politics,
r/economics, $\dots$). This means that
in the datasets built from this social media the contents $\mathcal{C} $ is the
set of URLs posted on these pages, which, differently from Twitter, most likely
come from different sources.

This posts are in turn crossposted, i.e. reposted on other subreddits. Each of
these \emph{crosspost} will correspond to another thread.

We also analyzed a
very specific case, that of r/asktrumpsupporters. This subreddit is a "Q\&A
subreddit to understand Trump supporters, their views, and the reasons behind
those views. Debates are discouraged" (from its description). We found it particularly interesting as it provides an explicit labeling of the
users who, before commenting in any of these posts, must "declare" their side
by choosing a \emph{flair}, which is shown next to the username of the person
commenting.  Three flairs are available: \emph{Trump Supporter}, \emph{Non
	supporter} and \emph{Undecided}.

The PRAW library is used for retrieving the data \cite{praw}.

\paragraph{Edge weights assignment.}%
\label{par:assigning_edge_weights}

Once the threads interactions are retrieved they are passed to a state of the
art sentiment analyzer which labels them. More specifically, the model used is
RoBERTa, adapted and retrained for dealing with Twitter
data \cite{Barbieri2020}. The model is made available by the Transformers
python library \cite{wolf-etal-2020-transformers}.

\bigskip

Finally, complying with the current privacy legislation, all the data related
to the user is pseudo-anonymized (accounts identifier are replaced by random
ones) while no data is publicly available.

\subsection{Initial observations on the datasets}%
\label{sub:some_observations_on_the_datasets}

Reddit and Twitter are intrinsically different social medias. As mentioned
before, Reddit focuses on subreddits, where all the discussions related to a
certain topic find their place and most of the users interested in the
theme gather; we think that this contributes to create community of users which are much more
active and likely to discuss among each other, as they end up looking at the
same posts and discussions.

Twitter, instead, is a much less "centralized" social media, where there a lot
of accounts with many followers, which often produces disjoint communities,
even if they discuss very similar topics.

The "centralization" of Reddit produces, in our data model, contents that are
associated with few threads. We observed that even the most discussed articles
on r/politics (a subreddit focusing on U.S.\ politics discussions) are
\emph{crossposted} one to five times (see, for example,
\autoref{fig:tex/img/reddit-crossposts}), while articles of the New York Times are
often shared even 100 times.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{tex/img/reddit-crossposts.png}
	\caption{The \emph{crossposts} on one of the most discussed article of the
		day on r/politics.}%
	\label{fig:tex/img/reddit-crossposts}
\end{figure}

\subsection{Synthetic data}%
\label{sub:synthetic_data}

Here we propose $2$ possible methods for generating data, the Signed SBM and
the Information spread Model.

\subsubsection{Signed SBM}%
\label{ssub:signed_sbm}

This model is very similar to the Stochastic Block Model (SBM), a model
commonly used for generating random graphs having some community structures
\cite{Newman2018}.

The Signed SBM is based on the following parameters:
\begin{itemize}
	\item $b_{i} $, the group assignment of each vertex $i$.
	\item $\omega ^{+} _{rs} $ and $\omega ^{-} _{rs} $, the probabilities
	      of positive and negative edges, respectively, between users in
	      group $r$ and $s$. Vertices have also a probability of not having an
	      edge, which is equal to $1 - \omega ^{-} _{rs} - \omega ^{+} _{rs} $.
	      For this reason it is needed that $\omega ^{+} _{rs} + \omega ^{-} _{rs} \leq 1$.
	\item $\theta \leq 1$, controlling the reduction of the probability of interacting
	      between \emph{inactive} communities
\end{itemize}

Therefore, generating a thread layer\footnote{in this model we will generate contents univocally associated to threads} for the \emph{Interaction Graph} involves the following process
\begin{enumerate}
	\item Sample $n'$ among the $n$ communities. These are the
	      \emph{active} communities in the thread.
	\item For each node pair $i, j$ consider their corresponding groups $r$ and
	      $s$ and, if both communities are \emph{active},
	      draw from
	      \begin{equation*}
		      (\omega _{rs} ^{+}, \omega _{rs} ^{-}, 1 - \omega _{rs} ^{+} - \omega _{rs} ^{-})
	      \end{equation*}
	      in order to add a positive,
	      negative or no edge.
	      Otherwise, if at least one of the
	      two communities is not \emph{active}, the distribution becomes
	      \begin{equation*}
		      (\theta \omega _{rs} ^{+}, \theta \omega _{rs} ^{-}, 1 - \theta
		      (\omega _{rs} ^{+} + \omega _{rs} ^{-})).
	      \end{equation*}

\end{enumerate}

\subsubsection{Information spread model}%
\label{ssub:information_spread_model}

Here we describe the Information spread model, which aims at simulating the
process of information flowing between different users of a social media.

Like in the Signed SBM, each node has a group assignment $b_i$ and
each pair of gruops has probabilities of
positive and negative edges ($\omega _{rs}^{+}  $ and $\omega _{rs}^{-}  $,
respectively, with $\omega ^{-} _{rs} + \omega ^{+} _{rs}
	\leq 1$).
Additionally, we have the following new parameters:

\begin{itemize}
	\item $\{\phi_{rs} \}$, the parameters of a standard SBM. These are used for
	      generating a graph $G_f$, which we will call the \emph{friend} graph,
	      representing the friendship relationships between the users. We will
	      refer to neighbours in this graph as \emph{friends}.
	\item $\beta _a$, the probability that a node is initially activated.
	\item $\beta _n$, the probability the an inactive node is activated
	      from an \emph{active} friend.
\end{itemize}

\bigskip
After generating $G_f$ from a SBM with parameters $\{\phi_{rs} \}$, the
generation of each thread of the \emph{Interaction Graph} goes as
follows:

\begin{enumerate}
	\item Activate each vertex with probability $\beta_{a}  $.
	\item Each of these active nodes activates its inactive friends with
	      probability $\beta_n$.
	\item Similarly to the Signed SBM, if two nodes are both active, draw
	      from
	      \begin{equation*}
		      (\omega _{rs} ^{+}, \omega _{rs} ^{-}, 1 - \omega _{rs} ^{+} -
		      \omega _{rs} ^{-})
	      \end{equation*}
	      for adding a positive, negative or no edge. If, instead, at least one
	      of them is not active, draw from
	      \begin{equation*}
		      (\theta \omega _{rs} ^{+}, \theta \omega _{rs} ^{-}, 1
		      - \theta (\omega _{rs} ^{+} + \omega _{rs} ^{-})).
	      \end{equation*}
\end{enumerate}

\section{Major results}

The following presented results have been obtained from a Python
implementation of the methods decribed in the previous chapters. The library used
for handling and manipulating graphs is graph-tool, which has been chosen
because of its efficiency \cite{peixoto_graph-tool_2014}.

\subsection{Initial Real-world data analysis}%
\label{sub:validity_problem_definition}

We did a initial analysis of the data to verify the validity of the problem.

We can gain some insights about the existance of echo chambers by comparing
the distribution of $\eta(C)$ and $\eta(T)$ for different interaction graphs.
% Threads with an high fraction of positive edges may correspond to echo
% chambers, i.e. users that discuss a topic with no \emph{controversy}.
% Consequently, if we find an increase in the relative number of threads with a
% low $\eta(T)$ (with respect to content distribution) this may mean that this
% effect is already visible.
Intuitively, echo chambers may correspond to threads with an
high fraction of positive edges. Consequently, we should expect an increase
for low values of $\eta(T)$ with respect to the $\eta(C)$ distribution.

We report these results for
three datasets, a first built over @nytimes, a second over @foxnews
and a third one over @bbcnews Twitter accounts\footnotemark (\autoref{fig:eta-content-thread}).

\footnotetext{As explained above (\autoref{par:twitter-data}), we are referring
	to the accounts that are used to retrieve the contents of the graph}

By looking at these histograms it is evident, as we were expecting, that when
moving from
contents to threads there is a significant increase in the percentage of
threads with a very small $\eta$, meaning that it is possible that contents which
globally have
a non negligible amount of negative edges produce also threads that have
very few or no negative edges. These are the subgraphs in which we expect to find the echo chambers.

\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/nytimes700/neg-fraction-content-hist.pdf}
			\caption{$\eta(C)$ of @nytimes}
			\label{fig:nytimes-content-eta}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/nytimes700/neg-fraction-thread-hist.pdf}
			\caption{$\eta(T)$ for @nytimes}
			\label{fig:nytimes-thread-eta}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/foxnews2000/neg-fraction-content-hist.pdf}
			\caption{$\eta(C)$ of @foxnews}
			\label{fig:foxnews-content-eta}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/foxnews2000/neg-fraction-thread-hist.pdf}
			\caption{$\eta(T)$ for @foxnews}
			\label{fig:foxnews-thread-eta}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/bbcnews500/neg-fraction-content-hist.pdf}
			\caption{$\eta(C)$ of @bbcnews}
			\label{fig:CNN-content-eta}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/bbcnews500/neg-fraction-thread-hist.pdf}
			\caption{$\eta(T)$ for @bbcnews}
			\label{fig:CNN-thread-eta}
		\end{subfigure}
		\caption{$\eta(C)$ and $\eta(T)$ distribution for many datasets.}
		\label{fig:eta-content-thread}
	\end{center}
\end{figure}

\bigskip
For verifying the reliability of the definition of \emph{controversial} content
we also looked at the fraction of negative edges for different datasets, each
associated to contents of the same topic, which we report in
\autoref{tab:eta-subreddits}.

This results show an intuitive associations between the fraction of negative edges in the
graph and the topic discussed:
graphs dealing with well-known \emph{controversial} contents, like r/politics and
r/asktrumpsupporters, are the one producing an higher fraction of negative
edges. Also, as expected, they are followed by related topics
(r/economics and r/climate) and football, while
subreddits in which discussions over technologies and sciences
predominate generally have less negative interactions between the users.

\begin{table}
	\centering
	\caption[Fraction of negative edges in different subreddits]{Fraction of negative edges for datasets build on different
		subreddits, each for $200$ contents.}
	\label{tab:eta-subreddits}
	{\small
		\begin{tabular}{c | p{6cm} | c}
			r/cats               & Pictures and videos about cats                & \num{0.16922540125610608} \\
			\hline
			r/Covid19            & Scientific discussion of the pandemic         & \num{0.2981398553220806}  \\
			\hline
			r/programming        & Computer Programming discussions              & \num{0.30264993026499304} \\
			\hline
			r/climate            & News about climate and related politics       & \num{0.391787072243346}   \\
			\hline
			r/Football           & News, Rumours, Analysis about \mbox{football} & \num{0.41103067250904934} \\
			\hline
			r/Economics          & News and discussion about economics           & \num{0.41730200715730514} \\
			\hline
			r/Politics           & News and discussion about U.S. politics       & \num{0.5112245929821013}  \\
			\hline
			r/AskTrumpSupporters & {Q\&A between Trump supporters and non
			supporters}          & \num{0.5329949238578681}                                                  \\
		\end{tabular}
	}
\end{table}

\bigskip
Furthermore, for each content $C$ in an interaction graph, we plotted the relationship between the sum of
the weights of its edges and its
$\eta(C)$ (see \autoref{fig:edge-sum-n-interactions}).

\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/cats200/edge-sum-n-interactions.pdf}
			\caption{r/cats}
			\label{fig:tex/out/cats200/edge-sum-n-interactions.pdf}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/covid19200/edge-sum-n-interactions.pdf}
			\caption{r/covid19}
			\label{fig:tex/out/covid19200/edge-sum-n-interactions.pdf}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/politics200/edge-sum-n-interactions.pdf}
			\caption{r/politics}
			\label{fig:tex/out/politics200/edge-sum-n-interactions.pdf}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/asktrumpsupporters200/edge-sum-n-interactions.pdf}
			\caption{r/asktrumpsupporters}
			\label{fig:tex/out/covid19200/edge-sum-n-interactions.pdf}
		\end{subfigure}
	\end{center}
	\caption[Sum edges over number of interactions for many datasets]{Plots of
		sum of the edges weights over the number of interaction for contents
		from different datasets/subreddits.}
	\label{fig:edge-sum-n-interactions}
\end{figure}

These plots confirm the previous observation and, more specifically, show that
the sum of the weights of the edges (which is closely related
to the $\eta(C)$ of $C$), for contents related to the same topic, is distributed
in a pattern which is very close to that of a line with rare or no outliers: as
we would intuitively say, contents related to politics are generally
controversial, and most of them have, as we can see in \autoref{fig:tex/out/politics200/edge-sum-n-interactions.pdf}, an high $\eta(C)$.
This is even more clearly visible when plotting the histogram of $\eta(C)$ for the contents in the dataset
(\autoref{fig:eta-distribution-content}), with most of the contents having a
$\eta(C)$ which is very close to the fraction of negative edges in the graph.

\begin{figure}
	\begin{center}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/cats200/neg-fraction-content-hist.pdf}
			\caption{r/cats}
			\label{fig:tex/out/cats200/neg-fraction-content-hist.pdf}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{tex/out/asktrumpsupporters200/neg-fraction-content-hist.pdf}
			\caption{r/asktrumpsupporters}
			\label{fig:asktrump-hist-eta}
		\end{subfigure}
	\end{center}
	\caption{$\eta(C)$ distribution for $2$ of the datasets shown in
		\autoref{fig:edge-sum-n-interactions}.}
	\label{fig:eta-distribution-content}
\end{figure}

%
% \subsection{Algorithms implementation efficiency}%
% \label{sub:algorithm_efficiency}

% todo:
% - time limitations of exact algorithms
% - describe beta algorithm and effect of beta, real world data
% - time efficiency of the greedy algorithms, real world data

\subsection{Tests on synthetic data}%
\label{sub:testing_on_synthetic_data}

For studying how the model behaves in controlled situations we define a
parametrized model based on the Information Spread model
(\autoref{ssub:information_spread_model}).

We choose $\beta _{a} = 1$, meaning that all nodes will be active in each
thread. This a
simplifying assumption which allows us to have a better grasp of the results.
Because of this choice the values $\beta _n = 1$, $\theta = 1$ and
\begin{equation}
	\phi_{rs}  =
	\begin{cases}
		1 \; & \text{if } r = s  \\
		0 \; & \text{otherwise }
	\end{cases}
\end{equation}
do not influence the structure of the model.

We also choose $\omega ^{-} _{rs}$ and $\omega ^{+} _{rs} $ to be dependant on
a \emph{noise} variable $x$
\begin{equation}
	\omega_{rs}^{+}   =
	\begin{cases}
		1 - x \;        & \text{if } r = s  \\
		\frac{x}{4}  \; & \text{otherwise }
	\end{cases}
\end{equation}
\begin{equation}
	\omega_{rs}^{-}   =
	\begin{cases}
		x \;                & \text{if } r = s  \\
		\frac{1 - x}{4}  \; & \text{otherwise }
	\end{cases}
\end{equation}

In absence of noise ($x = 0$) we will generate threads whose communities are
positive cliques and all the edges between vertices in different communities
are negative.

We will compare different techniques for finding echo chambers (which,
in this case, we will consider as corresponding to a community). The general
approach involves calling an algorithm (generally any of the methods presented
in \autoref{ch:solving}) returning a set of users $U \subseteq V$ which will be
labeled according to the majority of its members (by looking at the
ground-truth assignment). Then, edges contributing to $\xi(U)$
are removed from the graph.  After repeating $n$ times this process, where $n$ is the number
of communities in the model, we compare the ground-truth labels and the
predictions through the Jaccard coefficient and the Adjusted RAND
index\footnotemark.

\footnotetext{The Adjusted RAND index is a measure of similarity between
	different clusterings. It is based on the RAND index, which compares
	the number of agreeing pairs in the $2$ solutions; the Adjusted one corrects
	the RAND Index "by chance", i.e. compares it to the expected index
	(i.e.\ of a random assignment). For more details please
	refer to \cite{CharuC.Aggarwal2013}}

What we expect is that, as the value of $x$ increases, the produced threads
will have generally more negative edges inside a community and more positive
edges between different communities, making it more difficult for our
algorithms to spot a set of vertices corresponding to a community.

% todo: add results and specify measures

We compared the scores obtained with the \acrshort{MIP} for the \acrshort{ECP}
(\autoref{sub:a_mip_model_for_the_ecp}), the Rounding algorithm
(\autoref{ssub:rounding_algorithm}) and the $2$ alternative problems on the
\acrshort{TPA} Graph (\autoref{ssub:the_tpa_graph}). Due to the presence of the
\acrshort{MIP} model this group of experiments have been carried out on very
small graphs.

We then focused on the performances of the Rounding Algorithm on bigger
datasets.

% - describe rounding algorithm: when does it fail?
% - observation on the results (how does it depend on the number of threads and
%   on the number of nodes in each community?

\subsection{Detecting real-world echo chambers}%
\label{sub:detecting_real_echo_chambers}

We measured the performances of our algorithm on real-world data by trying to
classify the nodes of labeled datasets, similarly to what has been done
with synthetic data.

More specifically, we focused on the r/arktrumpsupporters and
@nytimes\footnotemark datasets (see
\autoref{sec:data_collection_and_generation} for details on how labels are
retrieved).

Both of them are unbalanced datasets, with r/asktrumpsupporters having a $79\%$
and $19\%$ of the users choosing the \emph{Non Supporter} and \emph{Trump
	Supporter} flairs, respectively (the remaining $2\%$ is
\emph{Undecided}), while $80\%$ of @nytimes users are labeled \emph{democrats}
and $20\%$ as \emph{republican}.

By looking at the poor results obtained with the rounding algorithm, which we show in
\autoref{tab:scores-datasets-labeled}, it is clear that the algorithm is not
able to correctly separate the communities. We motivate this with the following
reasons:
\begin{itemize}
	\item \textbf{Complexity of sentiment analysis of social media language.}
	      Social medias often involve messages which are not easily classifiable
	      as either friendly and hostyle, both because persons often use jargon
	      and because sometimes messages are aided by pictures and GIF which
	      are not analyzed by the Sentiment Analyzer.
	      % additional noise is introduced by messages in other languages as well
	      % as messages involving medias like pictures and GIF.
	\item \textbf{Non validity of the data model.} In trying to classify the
	      nodes with our \acrshort{ECP} solver we are supposing that the data
	      contains a clear separation of the users, in which one chamber
	      corresponds to a single community. Furthermore, we are assuming that
	      there are only two communities in the datasets we chose, which may also
	      be a limiting assumption, since
	      \begin{itemize}
		      \item @nytimes may contain echo chambers related to different
		            topics, as the set of contents does not only take into account
		            U.S. political discussion.
		      \item r/asktrumpsupporters may be a non-representative
		            dataset of discussion of polarized communities (we will
		            discuss this more in details in
		            \autoref{sec:the_r_asktrumpsupporters_case})
	      \end{itemize}
	\item \textbf{Limitations of the rounding algorithm.} Since we are
	      using an approximation algorithm we are not solving (exactly) the
	      \acrshort{ECP}: this may introduce limitations to the solutions which
	      is used to cluster the nodes.

	      More specifically, since at each iteration it uses a set of users
	      connected by positive edges as possible solution (see
	      \autoref{ssub:rounding_algorithm}) it is likely to return
	      a set $U$ with just one connected component.

\end{itemize}

\begin{table}
	\centering
	\caption{Classification scores on two labeled datasets}
	\label{tab:scores-datasets-labeled}
	\begin{tabular}{c|c|c}
		dataset                     & Adjusted RAND index       & Jaccard score             \\
		\hline r/asktrumpsupporters & \num{0.09453599837921367} & \num{0.01607717041800643} \\
		@nytimes                    & \num{0.02176064966494696} & \num{0.4200626959247649}  \\
	\end{tabular}
\end{table}

\footnotetext{For this dataset, in order to decrease the sparsity, we selected
	the 4-core}

\section{A study on r/asktrumpsupporters}%
\label{sec:the_r_asktrumpsupporters_case}

During the research we studied the r/asktrumpsupporters subreddit to
understand if it is possible to infer the community\footnotemark of the users by
looking at how they react to contents. More specifically, in a highly polarized
environment we expect that members in the same community of the
author have a positive stance towards the content; conversely, members of the
other communities have a negative one.

\footnotetext{In this case, we refer to community as the set of users with the same \emph{flair}}

\bigskip

For this analysis we add to our interaction graph another type of vertices, the
\emph{content nodes} which we univocally associate to a content. In order to
add links between users and contents, we consider the sequence of comments
leading to a specific user comment and multiply the sign of the
associated edges to calculate the sign of the content-user edge.

For example, consider a user $v_i$
replying positively to a post related to content $C$ and user $v_j$ replying
negatively to $v_i$. We will add a positive edge between $v_i$ and $C$ and a
negative edge between $v_j$ and $C$.

We then assign to the users linked to a content the same label of the author of
the post if the user is connected to the content
by a positive edge, the opposite one\footnotemark otherwise. Then, we measure the accuracy of
this classification.

\footnotetext{We ignore the \emph{Undecided} label}

We show in \autoref{fig:tex/out/experimental200/experimental200-accuracy-hist}
the histogram of the accuracy of classification of the contents in the dataset.
We see that in very few cases it is possible to discriminate users better than
by just using the majority label, while a big part of the contents achieve an
accuracy between $0.5$ and $0.6$.

\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{tex/out/experimental200/experimental200-accuracy-hist.pdf}
	\caption{Content accuracy in classifying r/asktrumpsupporters users.}%
	\label{fig:tex/out/experimental200/experimental200-accuracy-hist}
\end{figure}

We explain this with the following reasons:
\begin{itemize}
	\item The majority of the posts in the subreddit are open questions, e.g.\ ``What
	      do you think of ...' and, similarly, ``What's your idea on...''. This
	      means that our initial hypothesis that positive and negative
	      stances can be used for inferring the positions of
	      the users is not correct: for this type of posts most of the users
	      will just answer with their opinion, without being either friendly of
	      hostile, since the user posting does not express his opinion
	      in most of the times.
	\item This community of users is not a reliable sample: people attending
	      the subreddit are the part of communities most open to discussion, for this
	      reasons we generally expect a less evident division in the opinions and
	      less hostility in the comments.
\end{itemize}

\section{Further discussion on the results}%
\label{sec:discussion}

We focused our analysis and research on the rounding algorithm. We did this
since we observed, when running the other approximation algorithms on smaller datasets
(with less than $2000$ nodes), that its time performances were better than that of the
other methods. We report in \autoref{tab:times-approximation} the execution
times on some datasets.

\begin{table}
	\centering
	\caption[Execution time of the approximation algorithms]{Execution time of
		the approximation algorithms. $\alpha$ is chosen to be the median of the
		$\eta(C)$ for each dataset.}
	\label{tab:times-approximation}
	\begin{tabular}{c|c|c|c}
		Dataset         & Rounding & Peeling & $\beta$ \\
		\hline
		@EMA\_News      &          &         &         \\
		@bbcsciencenews &          &         &         \\
		@BBCSport       &          &         &         \\
		@BBCNewsEnts    &          &         &         \\
		@BBCTech        &          &         &         \\
		r/cats          &          &         &         \\
		r/covid19       &          &         &         \\
	\end{tabular}
\end{table}

Similarly, due to evident complexity limitations we were able to execute the exact
\acrshort{MIP} only on very small synthetic datasets, as shown in
\autoref{sub:synthetic_data}.

\bigskip

Our rounding algorithm achieves good performances on data with
polarized communities, showing also to be more and more robust as the
available data increases: a larger number of nodes and threads, as we discussed
in \autoref{sub:synthetic_data}, helps the algorithm selecting
\emph{intra-community} edges, which allows it to correctly classify the nodes.

Conversely, this algorithm was not able to produce a good clustering of the
nodes in real-world data. We need to add to the discussed motivations in
\autoref{sub:detecting_real_echo_chambers} another fundamental problem of the
datasets built from social medias: its instrinsic sparsity. We observed,
especially on Twitter, that even increasing the number of contents of the graph
does not produce denser graph, as the average degree remains between one and
two. This, as we have seen in the tests carried on synthetic data, is a limitation
on the robustness of the results. Also, analyzing the denser part of the
subgraph, a k-core, may affect the results since we expect that the echo chamber
effect is especially visible in small and isolated components, maybe a small
"bubble" of users sharing the same opinion, which may get excluded by
the k-core selection.
